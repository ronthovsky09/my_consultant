# Music-Consultant
Archive for a small data science project for statistics about music trends and insight on your music

Aayusha Shrestha, 
Yefan Zong, 
Matthew Tung, 
Kelsey Tsuchiyama 


**Music Consulting Project**
	
  Music consultant is a platform with two major components: it provides data about music properties based on the Spotify API and allows users to input information about their song to get feedback. Users can look at trends in and common values of attributes such as the danceability, tempo, energy, etc. of songs. They can also enter the attributes of their song into the platform to get insight and predictions on whether or not they reach their target emotion and genre, and how they stand among popular songs.
  
**Data Source:**
	Initially we developed our dataset using the top songs from the past ten years. We webscraped the Billboard website to retrieve the year-end charts with the top 100 songs for each of the past ten years. We then used the LyricsGenius API to add lyrics to our dataset that we could then run through IBM’s Watson API to perform sentiment analysis and add the resulting score to our dataset. At the end of this process we had ten different datasets with the song titles, artists, lyrics, and sentiment analysis scores that we uploaded to a SQL database. However, we found that the dataset was too small to look at trends and was biased as we were only looking at the top songs. 
	We then decided to use a larger dataset from Kaggle that had approximately 600,000 Spotify tracks from 1922-2021. This dataset had audio features that are available through Spotify’s API (i.e., acousticness, danceability, energy, etc.); however, it did not have lyrics nor sentiment analysis scores so we would have to add these to the dataset. Additionally, there were challenges as this dataset had tracks in many languages, some of which were not supported by IBM’s API, and tracks that weren’t songs (e.g., audio books). We decided to focus on tracks from 2000-2020 and filtered the dataset by release date to create separate datasets for each year to make it more manageable for our functions. Each year had around 10,000 tracks, so we took a sample of 2,000 for each year. We knew that our final datasets would be smaller than 2,000 after getting rid of the songs that did not have lyrics available in the Genius library, tracks that weren’t songs, and songs in languages not supported by IBM’s API. We concatenated the individual datasets into one before uploading it to our SQL database. We also dropped the lyrics column of our dataset before uploading as we had already performed our analysis on this component. 
  
**Visualization:**
  We first generate the histograms of the song’s sentiment analysis, danceability, acousticness, energy, instrumentalness, liveness, loudness, speechiness, tempo, valence against their frequency. We can get much information from these graphs, for example, most songs have either a score that’s near -1 or near 1 for sentiment analysis. There are only a few neutral songs with sentiment scores in the range of -0.25 and 0.25 which suggests when writing the lyric of a song, you want your song to have a clear and defined emotion to it. 
  Next, we create a line chart that shows the song trends over time. We can see from 1920 to around 1970, there’s a lot of variability in the data and most of the song’s elements change dramatically during this period. Therefore, in order to get better predictions for artists nowadays, we choose to look at data from 2000 and forward. We can see that since 2000, each element of the song is much more stable within a small range. For example, acousticness is at around 0.3; danceability is at around 0.6 but gradually increases during the past couple of years; energy fluctuates between 0.6 and 0.7 and so on. Based on the information, we can provide consultancy for artists regarding each aspect of their songs.
  After looking at the trend over the year, we run regression on these factors to see if there’s any obvious relationships between them and create a matrix graph.  For most pairs of data, there doesn’t seem to be much going on. However, there are some pairs of data that do show some relationships between the data. Take valence and danceability for example. There exists a positive linear relationship between them, meaning a song with higher danceability score generally has higher valence too. Therefore, if someone wants to increase their song’s valence and make their song more positive, they can do it by increasing the danceability of their song. 

**Classification:**
	The objective of classification is to use Spotify data as well as Spotify genre data to see if it can classify genres for 2000-2020 songs. In this process, we must first determine which numbers are the most important for classification. Spotify offers: acousticness, danceability, duration (in ms), energy, instrumentalness, liveness, loudness, speechiness, tempo, valence, and keys/mode. However, not every category contributes to genre classification. Thus, by looking at the histograms, we must first see if there are similarities between each category between 2000-2020 and genre. We found out that there were significant differences in keys, duration, and valence had significant different distributions. Not a surprise, keys and durations are significantly different because they often should not determine what genre the music is. In very special cases, duration may affect, especially with classical works, which tend to be longer music. It is interesting that valence does not share similar distributions. Valence indicates the “musical positiveness”, which can make sense because genres are not defined by the mood of the music, but rather the style of the work itself. 
	We first used SVC to learn the genre data, learning the data from numerous subgenres into a few main genres. By converting that, we borrowed Kaggle’s Regex codes. Spotify had numerous redundant genres that were categorized as an individual data entry, such as “Classical piano” and “Classical piano duo”, which should be categorized as one classical genre. Thus, by organizing the data using regex and average the bigger categories of data. 
	After categorizing the subgenre into 11 main genres, we use sklearn’s Support Vector Classification to learn the data. Afterwards, we use the learned algorithm to classify the popular songs from 2000-2020. Interestingly, sklearn only categorized the songs into 4 different main genres: Classical, Jazz, Pop, Rock, where rock was the most popular. Originally, our team thought that this might be due to the limited number of data it is categorizing. 

Thus, we tried to categorize all 100 years of popular music. 
Both rock and pop are still popular over 100 years and sklearn did not categorize with other genres. Thus, we tried deleting pop and rock to see if sklearn will diversify its classification. 

SVC is only classifying the songs into four different genres, where Funk got the most classification. This indicates two things: that funk may have a close relationship with rock and pop and the data available was not enough to distinctively classify genres. Therefore, we need to diversify the types of quantitative data  that will allow SVC to better find the multi-dimensional vector line and separate the genres. 
  
**User Interaction and Displaying Statistics:**
The results of our data gathering and analysis and the user interaction component can be found on our website. The website has an about section which tells you more about the project, a statistics page where you can find a summary of our results, some data visualizations, and a sample of our data, and finally, a user interaction page called My Consultant. This page asks the user to input different attributes of their song, and uses this information to give the user feedback. The feedback consists of information about the user’s song’s sentiment analysis, genre prediction, emotion prediction, and information about where the user stands in comparison to the data we have gathered. It also tells the user whether they reached their target genre and emotion. The website is essentially a combination of everything we’ve done so far: it brings together both methods and results we generated throughout the course of the semester. 

**IMPORTANT NOTE:**
Access to SQL databases and APIs is restricted due to the need of access to private servers and unique/personal tokens for API Calls.




